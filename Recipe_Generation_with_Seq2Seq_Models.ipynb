{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/STSBIZ/documentation/blob/master/Recipe_Generation_with_Seq2Seq_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qxy3-Y3rdO2_"
      },
      "source": [
        "\n",
        "####  Abdul Sabry   \n",
        "####  Learining how Recipe Generation with Seq2Seq Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction"
      ],
      "metadata": {
        "id": "YsREVvu8PE9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recipe Generation Model Training\n",
        "In this notebook, we train two types of sequence-to-sequence models to convert ingredients into cooking recipes. The models are:\n",
        "1. A basic sequence-to-sequence model without attention.\n",
        "2. An enhanced sequence-to-sequence model with attention.\n",
        "\n",
        "The goal is to explore how the inclusion of attention mechanisms affects the performance and quality of the generated recipes.\n"
      ],
      "metadata": {
        "id": "ktPsODDxYpum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Setup and Imports"
      ],
      "metadata": {
        "id": "oazGbJnuPI38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries if not already installed (uncomment if needed)\n",
        "# !pip install torch numpy matplotlib nltk\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.manual_seed(0)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(0)\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Check if CUDA is available and set device to GPU if it is\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOGNG0nr_sUq",
        "outputId": "ab3c3244-32bb-4db2-a959-1f5bb6877eca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Loading\n",
        "Here we load the dataset from the provided directories. We assume the data is split into training, development, and testing sets.\n"
      ],
      "metadata": {
        "id": "zS-JLK4dMCdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Cooking_Dataset/train'\n",
        "dev_dir = '/content/drive/MyDrive/Colab Notebooks/Cooking_Dataset/dev'\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Cooking_Dataset/test'\n",
        "\n",
        "# Function to load data\n",
        "def load_data(directory):\n",
        "    texts = []\n",
        "    for filename in os.listdir(directory):\n",
        "        full_path = os.path.join(directory, filename)\n",
        "       # print(f\"Checking file: {full_path}\")  # Debug: print the file path being checked\n",
        "        if os.path.isfile(full_path):\n",
        "           # print(f\"Loading file: {full_path}\")  # Debug: confirm the file is being loaded\n",
        "            with open(full_path, 'r', encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "                if content:  # Check if file is not empty\n",
        "                    texts.append(content)\n",
        "\n",
        "    return texts\n",
        "\n",
        "# Load datasets\n",
        "train_data = load_data(train_dir)\n",
        "dev_data = load_data(dev_dir)\n",
        "test_data = load_data(test_dir)\n"
      ],
      "metadata": {
        "id": "2RpyyakobQkY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to print first few entries of the dataset to verify loading\n",
        "def print_sample_data(data, num_samples=1):\n",
        "    for i in range(min(num_samples, len(data))):\n",
        "        print(f\"Sample {i+1}:\")\n",
        "        print(data[i][:500])  # Print first 500 characters of each sample\n",
        "        print(\"...\" + \"\\n\" * 2)  # Add ellipsis and space between samples\n",
        "\n",
        "# Example usage:\n",
        "print(\"Train Data Samples:\")\n",
        "print_sample_data(train_data)\n",
        "\n",
        "print(\"Development Data Samples:\")\n",
        "print_sample_data(dev_data)\n",
        "\n",
        "print(\"Test Data Samples:\")\n",
        "print_sample_data(test_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ls72oeT_QiBW",
        "outputId": "98f5f5e3-bd05-4f31-d3bc-9496643efaea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Samples:\n",
            "Sample 1:\n",
            "Title: basic biscotti\n",
            "categories:\tcookies\titalian\n",
            "servings: 36 servings\n",
            "ingredients: 4 oz blanched almonds\t2 1/2 c  flour\t1 3/4 c  granulated sugar\t1/4 ts salt\t1/4 ts baking soda\t3    eggs\n",
            "spread almonds on a baking sheet and toast them in oven until lightly golden .\n",
            "let cool .\n",
            "coarsely chop half the nuts .\n",
            "butter 2 large baking sheets .\n",
            "mix flour , sugar , salt and baking soda .\n",
            "beat in eggs , then whole and chopped nuts .\n",
            "mix to obtain a firm dough .\n",
            "knead briefly , then divide dough into 2 pi\n",
            "...\n",
            "\n",
            "\n",
            "Development Data Samples:\n",
            "Sample 1:\n",
            "Title: hazelnut fudge\n",
            "categories:\tcandy\n",
            "servings: 1 batch\n",
            "ingredients: 3 c  sugar\t1 c  milk\t1/2 c  corn syrup\t3 oz unsweetened chocolate\t1 c  butter\t2 ts vanilla\t1 c  oregon hazelnuts\n",
            "cook sugar , milk , corn syrup and butter to 238 .\n",
            "pour into mixing bowl ;\n",
            "add vanilla ;\n",
            "cool 15 minutes .\n",
            "beat until thick .\n",
            "stir in nuts and pour into buttered pan .\n",
            "hazelnut industry and the hazelnut marketing board\n",
            "END RECIPE\n",
            "\n",
            "Title: hazelnut waffles\n",
            "categories:\tbreads\tbreakfast\n",
            "servings: 4 servings\n",
            "ingredients\n",
            "...\n",
            "\n",
            "\n",
            "Test Data Samples:\n",
            "Sample 1:\n",
            "Title: byzantine dolmathes stuffed grapeleaves\n",
            "categories:\tgreek\tappetizers\n",
            "servings: 60 servings\n",
            "ingredients: karen mintzias\t1    jar grapeleaves (or fresh)\t2 tb oil\t1 lb ground beef or lamb\t2    onions; chopped\t1    garlic clove; pressed\t2 c  water\t1/2 c  tomato sauce\t1 c  rice\t2 tb chopped mint\t2 tb chopped parsley\t1/2 ts salt\tpepper to taste\t1/8 ts cinnamon\t1/2 c  currants\t1/4 c  port wine (optional)\t1/4 c  pine nuts or walnuts\t2 c  water\t1    lemon (juice only)\t3    eggs\t2    lemons (staine\n",
            "...\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "Data preprocessing is crucial for transforming raw text data into a structured format suitable for training machine learning models. This section includes:\n",
        "1. **Tokenization**: Converting sentences into words or meaningful tokens.\n",
        "2. **Vocabulary Building**: Creating a comprehensive list of unique words used across the training dataset.\n",
        "3. **Text to Sequence Conversion**: Transforming textual data into numerical form using the vocabulary index.\n"
      ],
      "metadata": {
        "id": "FBuBUHXLS8_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data loading and preprocessing function\n",
        "def load_and_preprocess_data(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    ingredients = []\n",
        "    recipes = []\n",
        "    for line in lines:\n",
        "        if 'ingredients:' in line:\n",
        "            ingredients.append(line.split('ingredients:')[1].strip())\n",
        "        elif 'recipe:' in line:\n",
        "            recipe_content = line.split('recipe:')[1].strip().replace('END RECIPE', '').strip()\n",
        "            recipes.append(recipe_content)\n",
        "\n",
        "    return ingredients, recipes\n"
      ],
      "metadata": {
        "id": "bx04XzcxLAf7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Building the Language Vocabularies\n",
        "Task: Construct vocabularies for both the ingredients and recipes which will include conversion from text to indices necessary for model processing."
      ],
      "metadata": {
        "id": "LC74payQLF1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.word2index = {}\n",
        "        self.index2word = {0: \"<sos>\", 1: \"<eos>\", 2: \"<pad>\"}\n",
        "        self.word2count = {}\n",
        "        self.n_words = 3  # Start counting from 3 to account for SOS, EOS, PAD\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.add_word(word)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.word2count[word] = 1\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n"
      ],
      "metadata": {
        "id": "iHdIsD7BLHnS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 1: Seq2Seq without Attention"
      ],
      "metadata": {
        "id": "AY9feq0WLNnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output, hidden = self.lstm(embedded, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.zeros(1, 1, self.hidden_size), torch.zeros(1, 1, self.hidden_size))\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.lstm(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.zeros(1, 1, self.hidden_size), torch.zeros(1, 1, self.hidden_size))\n"
      ],
      "metadata": {
        "id": "hz1mkW4oLOxC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_from_directory(directory_path):\n",
        "    \"\"\" Loads data from all files in the specified directory and extracts relevant content. \"\"\"\n",
        "    ingredients_list = []\n",
        "    recipes_list = []\n",
        "    for subdir, _, files in os.walk(directory_path):\n",
        "        for file in files:\n",
        "            if file.endswith('.txt'):\n",
        "                file_path = os.path.join(subdir, file)\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    text = f.read()\n",
        "                ingredients, recipe = extract_ingredients_and_recipe(text)\n",
        "                if ingredients and recipe:\n",
        "                    ingredients_list.append(ingredients)\n",
        "                    recipes_list.append(recipe)\n",
        "    return ingredients_list, recipes_list\n",
        "\n",
        "# Specify the paths to your dataset directories\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Cooking_Dataset/train'\n",
        "dev_dir = '/content/drive/MyDrive/Colab Notebooks/Cooking_Dataset/dev'\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Cooking_Dataset/test'\n",
        "\n",
        "# Load the datasets\n",
        "train_ingredients, train_recipes = load_data_from_directory(train_dir)\n",
        "dev_ingredients, dev_recipes = load_data_from_directory(dev_dir)\n",
        "test_ingredients, test_recipes = load_data_from_directory(test_dir)\n"
      ],
      "metadata": {
        "id": "XemUVR5RZ4-a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "bd7edd45-f0ad-4bf8-d790-a80533b71abd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'extract_ingredients_and_recipe' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f2fbfbceef7a>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Load the datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrain_ingredients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_recipes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mdev_ingredients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_recipes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mtest_ingredients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_recipes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-f2fbfbceef7a>\u001b[0m in \u001b[0;36mload_data_from_directory\u001b[0;34m(directory_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mingredients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_ingredients_and_recipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mingredients\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrecipe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mingredients_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mingredients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'extract_ingredients_and_recipe' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Before training our models, it's crucial to convert the raw text data into a format that can be efficiently processed by the neural network. This involves:\n",
        "- **Tokenization**: Converting text into sequences of integers.\n",
        "- **Building Vocabulary**: Creating a mapping from words to unique indices.\n",
        "- **Padding**: Standardizing the length of each sequence to allow batching of data.\n",
        "\n",
        "We will preprocess both the ingredients and recipe texts separately to maintain their distinct characteristics.\n"
      ],
      "metadata": {
        "id": "-tpN_shAMLn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data loading and preprocessing function\n",
        "def load_and_preprocess_data(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    ingredients = []\n",
        "    recipes = []\n",
        "    for line in lines:\n",
        "        if 'ingredients:' in line:\n",
        "            ingredients.append(line.split('ingredients:')[1].strip())\n",
        "        elif 'recipe:' in line:\n",
        "            recipe_content = line.split('recipe:')[1].strip().replace('END RECIPE', '').strip()\n",
        "            recipes.append(recipe_content)\n",
        "\n",
        "    return ingredients, recipes\n",
        "\n"
      ],
      "metadata": {
        "id": "kMkjc8n6vmuT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture\n",
        "\n",
        "We implement a Sequence to Sequence (Seq2Seq) model, which is composed of two main components:\n",
        "\n",
        "1. **Encoder**: Processes the input sequence and compresses the information into a context vector.\n",
        "2. **Attention Mechanism** (optional): Helps the decoder focus on relevant parts of the input sequence for better translation accuracy.\n",
        "3. **Decoder**: Generates the output sequence from the context vector.\n",
        "\n",
        "Both the encoder and decoder are based on the LSTM architecture, which is suitable for handling sequences due to its ability to maintain long-term dependencies.\n"
      ],
      "metadata": {
        "id": "RIbNpbiZwCBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output, hidden = self.lstm(embedded, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "# Assuming the decoder follows a similar pattern, with potential additions for attention\n"
      ],
      "metadata": {
        "id": "ljvYrA36wF-W"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder with Attention\n",
        "\n",
        "The attention mechanism allows the decoder to focus on different parts of the input sequence for each step of the output sequence. This is particularly useful in tasks like translation, where different parts of the input are relevant at different stages of decoding.\n"
      ],
      "metadata": {
        "id": "BMj1VtzGwOkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=10):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.lstm(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
      ],
      "metadata": {
        "id": "UkxCtiydwRdz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Configuration\n",
        "\n",
        "Set up the optimizer, loss function, and define the training loop. We will use the Cross-Entropy Loss and the Adam optimizer for training our Seq2Seq model.\n"
      ],
      "metadata": {
        "id": "SmD73HbRwVUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the maximum length of the sequences your model will handle\n",
        "MAX_LENGTH = 10  # You can adjust this based on your specific dataset or requirements\n"
      ],
      "metadata": {
        "id": "wz9EyVExwoSs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
        "    max_length = MAX_LENGTH  # Use the global variable\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < 0.5 else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, _ = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, _ = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n"
      ],
      "metadata": {
        "id": "XQGU60GtwXhK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation\n",
        "\n",
        "### Training Loop\n",
        "The training loop will run for a predefined number of epochs. During each epoch, the model will train on the entire training dataset and then validate its performance on the development set. This allows us to monitor the model's ability to generalize to new data and adjust training parameters accordingly.\n",
        "\n",
        "### Evaluation Metrics\n",
        "We will use metrics such as the loss and potentially the BLEU score for more insightful evaluation of the translation quality. These metrics will help us understand how well the model is performing and where it might need adjustments.\n"
      ],
      "metadata": {
        "id": "T8vc5KEZw3uD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "    # Tokenize the lines\n",
        "    seq = tokenizer.texts_to_sequences(lines)\n",
        "    # Pad the sequences with zeros\n",
        "    seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "    return seq\n",
        "\n",
        "# Choose a consistent sequence length based on your data analysis\n",
        "max_length_ingredients = 50\n",
        "max_length_recipes = 150\n",
        "\n",
        "# Create tokenizer\n",
        "ingredients_tokenizer = create_tokenizer(train_ingredients)\n",
        "recipes_tokenizer = create_tokenizer(train_recipes)\n",
        "\n",
        "# Prepare training, validation, and test data\n",
        "trainX = encode_sequences(ingredients_tokenizer, max_length_ingredients, train_ingredients)\n",
        "trainY = encode_sequences(recipes_tokenizer, max_length_recipes, train_recipes)\n",
        "devX = encode_sequences(ingredients_tokenizer, max_length_ingredients, dev_ingredients)\n",
        "devY = encode_sequences(recipes_tokenizer, max_length_recipes, dev_recipes)\n",
        "testX = encode_sequences(ingredients_tokenizer, max_length_ingredients, test_ingredients)\n",
        "testY = encode_sequences(recipes_tokenizer, max_length_recipes, test_recipes)\n"
      ],
      "metadata": {
        "id": "eyzvQs99cgrV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "0b4d95e1-087a-437e-8a6c-ae70fdf7a24e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_ingredients' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-dadb12b3c453>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Create tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mingredients_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ingredients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mrecipes_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_recipes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_ingredients' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ingredients_tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "sgJmbCeQyv9f",
        "outputId": "dee81094-c02d-4a04-8829-a469e18dc47d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ingredients_tokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ad0e1743d977>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mingredients_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ingredients_tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Model Building\n",
        "\n",
        "In this section, we will define two types of Seq2Seq models:\n",
        "1. **Seq2Seq Model without Attention**: This traditional model architecture uses an encoder to compress all input information into a context vector, which the decoder then uses to reconstruct the output sequence.\n",
        "2. **Seq2Seq Model with Attention**: This enhanced model architecture allows the decoder to focus on different parts of the input sequence during the decoding process, which helps in handling long input sequences effectively and improves model performance by retaining more contextual information.\n"
      ],
      "metadata": {
        "id": "5maAHuRFfvs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hidden_dim, dropout):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(1)  # Adjusting for batch_first=True in LSTM\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(1))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hidden_dim, dropout):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(1)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(1))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hidden_dim, dropout):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.attn_combine = nn.Linear(hidden_dim * 2, emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        input = input.unsqueeze(1)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        attn_weights = torch.softmax(self.attn(torch.cat((embedded, hidden), dim=2)), dim=2)\n",
        "        attn_applied = torch.bmm(attn_weights, encoder_outputs)\n",
        "        output = self.attn_combine(torch.cat((embedded, attn_applied), dim=2))\n",
        "        output, (hidden, _) = self.lstm(output, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(1))\n",
        "        return prediction, hidden, attn_weights\n"
      ],
      "metadata": {
        "id": "zW9jSTvfL8YU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Setup\n",
        "\n",
        "For training our Seq2Seq models, we'll use the following configuration:\n",
        "- **Optimizer**: Adam, with a learning rate of 0.001. Adam is chosen for its adaptive learning rate properties, which makes it suitable for this task.\n",
        "- **Loss Function**: CrossEntropyLoss, which is standard for classification tasks.\n",
        "- **Hyperparameters**:\n",
        "  - **Learning Rate**: 0.001\n",
        "  - **Hidden Dimensions**: 256\n",
        "  - **Embedding Dimensions**: 256\n",
        "  - **Dropout**: 0.1, to help prevent overfitting\n",
        "  - **Epochs**: 10\n",
        "  - **Batch Size**: 32, a standard size that balances speed and performance.\n",
        "  - **Clip**: 1, to prevent gradient explosion.\n"
      ],
      "metadata": {
        "id": "Av9VYS9MnntF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, (src, trg) in enumerate(iterator):\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg[:-1])\n",
        "\n",
        "        # Assume output shape is [trg_len, batch_size, output_dim]\n",
        "        output_dim = output.shape[-1]  # This should correctly fetch output_dim\n",
        "\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n"
      ],
      "metadata": {
        "id": "KVeNnNnLnmxz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Decoder output dimension:\", model.decoder.output_dim)\n"
      ],
      "metadata": {
        "id": "Q3shMkv2xZu1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "5ab55e10-8430-429d-ca91-16bda30d28e8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-5881e2986fb6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Decoder output dimension:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute Training and Monitor Progress\n",
        "\n",
        "Now that our models are set up and the training configurations are defined, we'll execute the training loops. We will train the models for a predetermined number of epochs and monitor the training progress. Additionally, we'll evaluate the performance on the development set after each epoch to monitor how well our models generalize over time.\n"
      ],
      "metadata": {
        "id": "cgbqeP2ZoesT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Load and prepare your dataset\n",
        "# This is a placeholder - you'll need to adapt this according to your actual data source and format.\n",
        "\n",
        "# Simulated data: Lists of indices representing words in a vocabulary\n",
        "ingredients_data = [torch.randint(0, 1000, (10,)).tolist() for _ in range(100)]  # 100 samples, each is a list of indices\n",
        "recipes_data = [torch.randint(0, 1000, (15,)).tolist() for _ in range(100)]      # Each recipe can have a different length\n",
        "\n",
        "# Ensure data is appropriate for your application and matches the expected input format for RecipeDataset.\n"
      ],
      "metadata": {
        "id": "iuUyvfhPucVJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class RecipeDataset(Dataset):\n",
        "    def __init__(self, ingredients, recipes):\n",
        "        self.ingredients = [torch.tensor(ing) for ing in ingredients]\n",
        "        self.recipes = [torch.tensor(rec) for rec in recipes]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ingredients)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.ingredients[idx], self.recipes[idx]\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        ingredients, recipes = zip(*batch)\n",
        "        ingredients_padded = pad_sequence(ingredients, batch_first=True, padding_value=0)  # Assuming 0 is PAD_IDX\n",
        "        recipes_padded = pad_sequence(recipes, batch_first=True, padding_value=0)\n",
        "        return ingredients_padded, recipes_padded\n"
      ],
      "metadata": {
        "id": "qtWSIijmuB1-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the dataset with your data\n",
        "dataset = RecipeDataset(ingredients_data, recipes_data)\n",
        "\n",
        "# Create a DataLoader\n",
        "train_iterator = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=RecipeDataset.collate_fn)\n"
      ],
      "metadata": {
        "id": "9BuTSndKtNRo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement Training and Evaluation Loops\n",
        "\n",
        "In this section, we'll implement the training and evaluation loops for our models. We'll train the models using the training dataset and periodically evaluate them on the development set to monitor performance and adjust our training strategy as needed. This approach helps in understanding the model's learning over time and ensuring it generalizes well to new, unseen data.\n"
      ],
      "metadata": {
        "id": "A0PubN8Q1ZXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, (src, trg) in enumerate(iterator):\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg[:-1])  # Assumes the model is set up for input-output offset\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].contiguous().view(-1, output_dim)\n",
        "        trg = trg[1:].contiguous().view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (src, trg) in enumerate(iterator):\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "            output = model(src, trg[:-1])  # Forward pass without teacher forcing\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].contiguous().view(-1, output_dim)\n",
        "            trg = trg[1:].contiguous().view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "# Train and evaluate the model\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, 1)  # Assume gradient clipping value is 1\n",
        "    valid_loss = evaluate(model, dev_iterator, criterion)\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\tValid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f}')\n"
      ],
      "metadata": {
        "id": "VOmpMnPohP5O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "18b86df7-a590-4c68-a5d7-ede48b3a445b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'NUM_EPOCHS' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-a6b9c4e22f27>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Train and evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assume gradient clipping value is 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'NUM_EPOCHS' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reporting and Visualization of Training Outcomes\n",
        "\n",
        "Upon completing the training and evaluation cycles, it's important to report and visualize the outcomes. We'll plot the training and validation losses to identify patterns such as overfitting or underfitting and discuss any potential improvements or adjustments needed.\n"
      ],
      "metadata": {
        "id": "dYNoP7V1vQml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assume train_losses and valid_losses are collected during the epochs\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(train_losses, label='Training loss')\n",
        "plt.plot(valid_losses, label='Validation loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-VnQHbu84nss",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "b461972f-22ba-4512-9e11-7fd701fd447b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_losses' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-457538a4b619>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Assume train_losses and valid_losses are collected during the epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}